{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "val dfFlight = spark.read.sqlanalytics(\"edw.dbo.hyssh_flighttransaction\") \r\n",
        "val dfweather = spark.read.sqlanalytics(\"edw.dbo.hyssh_weather\")\r\n",
        "val dfairport = spark.read.sqlanalytics(\"edw.dbo.hyssh_airport\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS ##REPLACETHIS##_flight_db\")\r\n",
        "spark.catalog.setCurrentDatabase(\"##REPLACETHIS##_flight_db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "dfFlight.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"flight_history\")\r\n",
        "dfweather.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"weather_history_curated\")\r\n",
        "dfairport.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"airport_master\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "select distinct  \r\n",
        "fhc.dep_delay_15,\r\n",
        "amo.airport_id as origin_airport_id,\r\n",
        "fhc.origin_airport_cd,\r\n",
        "amo.display_airport_name as origin_airport_nm,\r\n",
        "amo.latitude as origin_airport_latitude,\r\n",
        "amo.longitude as origin_airport_longitude,\r\n",
        "fhc.month as flight_month,\r\n",
        "fhc.day_of_month as flight_day_of_month,\r\n",
        "fhc.day_of_week as flight_day_of_week,\r\n",
        "fhc.crs_dep_tm as flight_dep_hour, \r\n",
        "fhc.carrier_cd,\r\n",
        "amd.airport_id as dest_airport_id,\r\n",
        "fhc.dest_airport_cd,\r\n",
        "amd.display_airport_name as dest_airport_nm,\r\n",
        "amd.latitude as dest_airport_latitude,\r\n",
        "amd.longitude as dest_airport_longitude,\r\n",
        "whc.wind_speed,\r\n",
        "whc.sea_level_pressure,\r\n",
        "whc.hourly_precip\r\n",
        "from \r\n",
        "flight_history fhc\r\n",
        "left outer join\r\n",
        "airport_master amo\r\n",
        "on (fhc.origin_airport_cd=amo.airport)\r\n",
        "left outer join\r\n",
        "airport_master amd\r\n",
        "on (fhc.dest_airport_cd=amd.airport)\r\n",
        "left outer join\r\n",
        "weather_history_curated whc\r\n",
        "on (whc.latitude=amo.latitude AND\r\n",
        "    whc.longitude=amo.longitude AND\r\n",
        "    fhc.day_of_month=whc.day AND\r\n",
        "    fhc.month=whc.month AND\r\n",
        "    fhc.crs_dep_tm=whc.hour)\r\n",
        "WHERE fhc.dep_delay_15 is not null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "val matViewDF = spark.sqlContext.sql(\"\"\"\r\n",
        "select distinct  \r\n",
        "fhc.dep_delay_15,\r\n",
        "amo.airport_id as origin_airport_id,\r\n",
        "fhc.origin_airport_cd,\r\n",
        "amo.display_airport_name as origin_airport_nm,\r\n",
        "amo.latitude as origin_airport_latitude,\r\n",
        "amo.longitude as origin_airport_longitude,\r\n",
        "fhc.month as flight_month,\r\n",
        "fhc.day_of_month as flight_day_of_month,\r\n",
        "fhc.day_of_week as flight_day_of_week,\r\n",
        "fhc.crs_dep_tm as flight_dep_hour, \r\n",
        "fhc.carrier_cd,\r\n",
        "amd.airport_id as dest_airport_id,\r\n",
        "fhc.dest_airport_cd,\r\n",
        "amd.display_airport_name as dest_airport_nm,\r\n",
        "amd.latitude as dest_airport_latitude,\r\n",
        "amd.longitude as dest_airport_longitude,\r\n",
        "whc.wind_speed,\r\n",
        "whc.sea_level_pressure,\r\n",
        "whc.hourly_precip\r\n",
        "from \r\n",
        "flight_history fhc\r\n",
        "left outer join\r\n",
        "airport_master amo\r\n",
        "on (fhc.origin_airport_cd=amo.airport)\r\n",
        "left outer join\r\n",
        "airport_master amd\r\n",
        "on (fhc.dest_airport_cd=amd.airport)\r\n",
        "left outer join\r\n",
        "weather_history_curated whc\r\n",
        "on (whc.latitude=amo.latitude AND\r\n",
        "    whc.longitude=amo.longitude AND\r\n",
        "    fhc.day_of_month=whc.day AND\r\n",
        "    fhc.month=whc.month AND\r\n",
        "    fhc.crs_dep_tm=whc.hour)\r\n",
        "WHERE fhc.dep_delay_15 is not null\r\n",
        "\"\"\").cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "matViewDF.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"##REPLACETHIS##_flight_db.hyssh_materialized_view\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "import org.apache.spark.mllib.regression.LabeledPoint \r\n",
        "import org.apache.spark.mllib.linalg.Vectors \r\n",
        "import org.apache.spark.mllib.util.MLUtils \r\n",
        "import org.apache.spark.ml.feature.StringIndexer \r\n",
        "import org.apache.spark.ml.feature.VectorAssembler \r\n",
        "import org.apache.spark.mllib.linalg.Vectors \r\n",
        "import org.apache.spark.storage.StorageLevel "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "val modelInputDF = spark.sqlContext.sql(\"\"\"\r\n",
        "select distinct \r\n",
        "cast(dep_delay_15 as double),\r\n",
        "cast(origin_airport_id as double),\r\n",
        "cast(flight_month as double),\r\n",
        "cast(flight_day_of_month as double),\r\n",
        "cast(flight_day_of_week as double),\r\n",
        "cast(flight_dep_hour as double),\r\n",
        "cast(dest_airport_id as double),\r\n",
        "cast(wind_speed as double),\r\n",
        "cast(sea_level_pressure as double),\r\n",
        "cast(hourly_precip as double) \r\n",
        "from materialized_view\r\n",
        "\"\"\").na.drop.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "val cols = Array(\"origin_airport_id\", \"flight_month\", \"flight_day_of_month\",\"flight_day_of_week\", \"flight_dep_hour\",  \"dest_airport_id\", \"wind_speed\",\"sea_level_pressure\",\"hourly_precip\")\r\n",
        "val datawithFeatures = new VectorAssembler().setInputCols(cols).setOutputCol(\"features\").transform(modelInputDF)\r\n",
        "\r\n",
        "val labelDf = new StringIndexer().setInputCol(\"dep_delay_15\").setOutputCol(\"label\").fit(datawithFeatures).transform(datawithFeatures)\r\n",
        "labelDf.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "import org.apache.spark.mllib.regression.LabeledPoint \r\n",
        "import org.apache.spark.mllib.util.MLUtils\r\n",
        "import org.apache.spark.ml.Pipeline\r\n",
        "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\r\n",
        "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\r\n",
        "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\r\n",
        "import org.apache.spark.ml.tuning.{ParamGridBuilder,CrossValidator}\r\n",
        "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "val Array(trainingDataset, testDataset) = labelDf.randomSplit(Array(0.75, 0.25), seed =1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "val randForest = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(100).setMaxDepth(10).setMaxBins(160)\r\n",
        "\r\n",
        "val model = randForest.fit(trainingDataset.toDF())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "// Make predictions on the test dataset\r\n",
        "val predictions = model.transform(testDataset)\r\n",
        "\r\n",
        "// Select example rows to display.\r\n",
        "predictions.select(\"prediction\", \"label\", \"probability\" ,\"features\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "// Select (prediction, true label) and compute test error.\r\n",
        "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\r\n",
        "\r\n",
        "val accuracy = evaluator.evaluate(predictions)\r\n",
        "println(\"Test Error = \" + (1.0 - accuracy))\r\n",
        "//0.794150404516767"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "//* Binary classification evaluation metrics*//\r\n",
        "val evaluator = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"probability\").setMetricName(\"areaUnderROC\")\r\n",
        "val ROC = evaluator.evaluate(predictions)\r\n",
        "println(\"ROC on test data = \" + ROC)\r\n",
        "//0.7348870570394918"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "//Print model \r\n",
        "println(\"Learned classification forest model:\\n\" + model.toDebugString)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "model.write.overwrite.save(\"abfss://sampledataset@REPLACETHIS.dfs.core.windows.net/model/hyssh/flightdelay\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "//FEATURE IMPORTANCE\r\n",
        "model.featureImportances\r\n",
        "\r\n",
        "//0 - origin_airport_id - 0.061617093986384194\r\n",
        "//1 - flight_month - 0.04194211627845236\r\n",
        "//2 - flight_day_of_month - 0.11509911942769702\r\n",
        "//3 - flight_day_of_week - 0.061531660019990865\r\n",
        "//4 - flight_dep_hour - 0.4371281475075368\r\n",
        "//5 - carrier_indx - 0.05770819357125712\r\n",
        "//6 - dest_airport_id - 0.01769814715174663\r\n",
        "//7 - wind_speed - 0.02170460496570623\r\n",
        "//8 - sea_level_pressure - 0.12510959002658545\r\n",
        "//9 - hourly_precip - 0.060461327064643425"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "Synapse Spark"
    },
    "language_info": {
      "name": "scala"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}